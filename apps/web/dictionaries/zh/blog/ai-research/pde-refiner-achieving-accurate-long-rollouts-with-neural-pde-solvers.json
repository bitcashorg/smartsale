{
  "relatedBlogs": [
    {
      "id": "198277124",
      "topics": ["大型语言模型", "金融", "医疗", "法律", "提示工程"],
      "title": "通过阅读理解适应大型语言模型",
      "slug": "adapting-large-language-models-via-reading-comprehension",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-27T03:34:22+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      },
      "seo": {
        "title": "通过阅读理解适应大型语言模型",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277342",
      "topics": ["大型语言模型", "多语言"],
      "title": "OpenBA:一个开源的150亿参数双语非对称seq2seq模型",
      "slug": "openba-an-open-sourced-15b-bilingual-asymmetric-seq2seq-model-pre-trained-from-sc",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-27T03:34:02+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      },
      "seo": {
        "title": "OpenBA:一个开源的150亿参数双语非对称seq2seq模型",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277117",
      "topics": ["大型语言模型", "检索"],
      "title": "PDFTriage:长结构化文档的问答系统",
      "slug": "pdftriage-question-answering-over-long-structured-documents",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-27T03:33:49+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      },
      "seo": {
        "title": "PDFTriage:长结构化文档的问答系统",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277138",
      "topics": ["大型语言模型", "微调"],
      "title": "排序LLaMA:通过排序微调释放大型语言模型中间层的动态推理潜力",
      "slug": "sorted-llama-unlocking-the-potential-of-intermediate-layers-of-large-language-mod",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-27T03:33:31+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      },
      "seo": {
        "title": "排序LLaMA:通过排序微调释放大型语言模型的动态推理潜力",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277150",
      "topics": ["大型语言模型", "指令调优", "多模态"],
      "title": "指令调优大型多模态模型扩展的实证研究",
      "slug": "an-empirical-study-of-scaling-instruct-tuned-large-multimodal-models",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-27T03:33:15+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      },
      "seo": {
        "title": "指令调优大型多模态模型扩展的实证研究",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277160",
      "topics": ["大型语言模型", "代理", "游戏"],
      "title": "心智代理:涌现的游戏交互",
      "slug": "mindagent-emergent-gaming-interaction",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-27T03:32:58+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692841427-researchpaper8a.png"
      },
      "seo": {
        "title": "心智代理:涌现的游戏交互",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277196",
      "topics": ["大型语言模型", "结构化数据"],
      "title": "Struc-Bench:大型语言模型真的擅长生成复杂结构化数据吗?",
      "slug": "struc-bench-are-large-language-models-really-good-at-generating-complex-structure",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-25T03:38:38+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      },
      "seo": {
        "title": "Struc-Bench:大型语言模型真的擅长生成复杂结构化数据吗?",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277217",
      "topics": ["大型语言模型", "隐私", "边缘计算"],
      "title": "利用大型语言模型从隐私保护掩码中恢复",
      "slug": "recovering-from-privacy-preserving-masking-with-large-language-models",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-25T03:38:26+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      },
      "seo": {
        "title": "利用大型语言模型从隐私保护掩码中恢复",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277239",
      "topics": ["大型语言模型", "聊天"],
      "title": "S3-DST:大型语言模型时代的结构化开放域对话分割和状态跟踪",
      "slug": "s3-dst-structured-open-domain-dialogue-segmentation-and-state-tracking-in-the-era",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-25T03:38:11+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      },
      "seo": {
        "title": "S3-DST:大型语言模型时代的结构化开放域对话分割和状态跟踪",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277253",
      "topics": ["大型语言模型", "音频"],
      "title": "利用大型语言模型增强口语理解的文本",
      "slug": "augmenting-text-for-spoken-language-understanding-with-large-language-models",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-25T03:37:49+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      },
      "seo": {
        "title": "利用大型语言模型增强口语理解的文本",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277277",
      "topics": ["大型语言模型", "压缩"],
      "title": "语言建模即压缩",
      "slug": "language-modeling-is-compression",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-25T03:37:32+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      },
      "seo": {
        "title": "语言建模即压缩",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277345",
      "topics": ["大型语言模型", "多语言"],
      "title": "百川2:开放大规模语言模型",
      "slug": "baichuan-2-open-large-scale-language-models",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-24T05:59:10+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      },
      "seo": {
        "title": "百川2:开放大规模语言模型",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277360",
      "topics": ["大型语言模型", "RLHF"],
      "title": "通过优势模型和选择性回顾稳定RLHF",
      "slug": "stabilizing-rlhf-through-advantage-model-and-selective-rehearsal",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-24T05:22:15+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      },
      "seo": {
        "title": "通过优势模型和选择性回顾稳定RLHF",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277446",
      "topics": ["大型语言模型", "幻觉"],
      "title": "验证链减少大型语言模型的幻觉",
      "slug": "chain-of-verification-reduces-hallucination-in-large-language-models",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-24T05:21:05+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      },
      "seo": {
        "title": "验证链减少大型语言模型的幻觉",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277458",
      "topics": ["大型语言模型", "幻觉", "实体", "结构化数据"],
      "title": "LMDX:基于语言模型的文档信息提取和定位",
      "slug": "lmdx-language-model-based-document-information-extraction-and-localization",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2024-05-24T05:20:31+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      },
      "seo": {
        "title": "LMDX:基于语言模型的文档信息提取和定位",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198277209",
      "topics": ["大型语言模型", "数据"],
      "title": "SlimPajama-DC:理解大型语言模型训练的数据组合",
      "slug": "accelerating-llm-inference-with-staged-speculative-decoding",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-04T23:43:32+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692841427-researchpaper8a.png"
      },
      "seo": {
        "title": "SlimPajama-DC:理解大型语言模型训练的数据组合",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198044929",
      "topics": ["大型语言模型", "数据", "代理"],
      "title": "推理型具身代理的数据源",
      "slug": "accelerating-llm-inference-with-staged-speculative-decoding",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-04T07:33:25+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692841427-researchpaper8a.png"
      },
      "seo": {
        "title": "推理型具身代理的数据源",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198044879",
      "topics": ["大型语言模型", "变换器", "可解释性"],
      "title": "稀疏自编码器在语言模型中发现高度可解释的特征",
      "slug": "accelerating-llm-inference-with-staged-speculative-decoding",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-04T07:23:08+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692841427-researchpaper8a.png"
      },
      "seo": {
        "title": "稀疏自编码器在语言模型中发现高度可解释的特征",
        "description": "摘要评论与评分\n"
      }
    },
    {
      "id": "198044746",
      "topics": ["大型语言模型", "变换器", "训练"],
      "title": "稀疏连接基础模型的缩放定律",
      "slug": "accelerating-llm-inference-with-staged-speculative-decoding",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-04T07:09:44+01:00",
      "description": "摘要评论与评分",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692841427-researchpaper8a.png"
      },
      "seo": {
        "title": "稀疏连接基础模型的缩放定律",
        "description": "摘要评论与评分\n"
      }
    }
  ],
  "blogContent": {
    "id": "198277109",
    "topics": ["大型语言模型", "推理"],
    "title": "对比解码提高大型语言模型的推理能力",
    "slug": "pde-refiner-achieving-accurate-long-rollouts-with-neural-pde-solvers",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2023-10-04T22:18:11+01:00",
    "description": "摘要评论和评分",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Published on Sep 16"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Authors:"
                    },
                    {
                      "url": "https://huggingface.co/seanobrienresearch",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Sean O'Brien"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/mikelewis0",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Mike Lewis"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Abstract"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "We demonstrate that Contrastive Decoding -- a simple, computationally light, and training-free text generation method proposed by Li et al 2022 -- achieves large out-of-the-box improvements over greedy decoding on a variety of reasoning tasks. Originally shown to improve the perceived quality of long-form text generation, Contrastive Decoding searches for strings that maximize a weighted difference in likelihood between strong and weak models. We show that Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA 2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in addition to improvements on a collection of other tasks. Analysis suggests that Contrastive Decoding improves over existing methods by preventing some abstract reasoning errors, as well as by avoiding simpler modes such as copying sections of the input during chain-of-thought. Overall, Contrastive Decoding outperforms nucleus sampling for long-form generation and greedy decoding for reasoning tasks, making it a powerful general purpose method for generating text from language models."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "url": "https://arxiv.org/abs/2309.09117",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "View arXiv page"
                        }
                      ]
                    },
                    {
                      "url": "https://arxiv.org/pdf/2309.09117",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "View PDF"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Commentary"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The paper titled \"Contrastive Decoding Improves Reasoning in Large Language Models\" presents an approach to improving text generation quality and reasoning capabilities in large language models."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Key Insights:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "numbered",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": ["strong"],
                              "value": "Contrastive Decoding"
                            },
                            {
                              "type": "span",
                              "value": ": This method leverages the difference in likelihood between strong and weak models to generate text. Originally designed for improving long-form text generation, the authors demonstrate its value for reasoning tasks as well."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": ["strong"],
                              "value": "Significant Outperformance"
                            },
                            {
                              "type": "span",
                              "value": ": Contrastive Decoding allows LLaMA-65B to surpass several other state-of-the-art models on specific reasoning benchmarks, such as the HellaSwag commonsense reasoning benchmark and the GSM8K math word reasoning benchmark."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": ["strong"],
                              "value": "Avoiding Errors"
                            },
                            {
                              "type": "span",
                              "value": ": The analysis indicates that this method can help in avoiding some abstract reasoning errors. It also reduces simpler errors such as unnecessary copying of input sections during text generation."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Potential Real-World Impact:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": ["strong"],
                              "value": "Enhanced Text Generation"
                            },
                            {
                              "type": "span",
                              "value": ": The method promises to improve the quality of text generated by large language models, making outputs more coherent, relevant, and reasoned."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": ["strong"],
                              "value": "Improved Reasoning"
                            },
                            {
                              "type": "span",
                              "value": ": A better performance on reasoning tasks can have numerous applications ranging from more intelligent chatbots to tools that can assist professionals in various analytical tasks."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": ["strong"],
                              "value": "Wider Applicability"
                            },
                            {
                              "type": "span",
                              "value": ": As a training-free method, Contrastive Decoding offers an advantage as it doesn't require additional computational resources for training."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": ["strong"],
                              "value": "Versatility"
                            },
                            {
                              "type": "span",
                              "value": ": The approach seems versatile, showing improvements across both long-form generation and specific reasoning tasks."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Challenges:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": ["strong"],
                              "value": "Dependence on Weak Models"
                            },
                            {
                              "type": "span",
                              "value": ": The effectiveness of Contrastive Decoding relies on the presence of both strong and weak models, which might not always be available or may vary in relative strength."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Given the novel approach to improve text generation and reasoning, as well as its demonstrated efficacy:"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "I'd rate the real-world impact of this paper as an 8.5 out of 10."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The method appears to offer a powerful, general-purpose technique for generating text from language models. If it can be broadly applied to a range of tasks and settings, its real-world impact could be considerable, especially in applications where reasoning capabilities of models are crucial."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "researchpaper9",
            "height": 816,
            "width": 1456,
            "filename": "researchpaper9.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "摘要评论和评分",
      "title": "对比解码提高大型语言模型的推理能力",
      "twitterCard": null,
      "image": {
        "width": 1456,
        "height": 816,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      }
    }
  },
  "topics": ["大型语言模型", "推理"]
}
